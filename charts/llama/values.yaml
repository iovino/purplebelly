replicaCount: 1

image:
  repository: ghcr.io/ggerganov/llama.cpp
  tag: server-cuda
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

resources:
  limits:
    nvidia.com/gpu: 1
  requests:
    cpu: 1
    memory: 2Gi

model:
  path: "/models/mistral-7b.Q4_K_M.gguf"
  storageClassName: llama-models
  size: 10Gi
